
# cost (成本函数)

$$
J(θ)=\frac{1}{2m}\sum_{i=1}^{m} {(h_θ(x^{(i)})-y^{(i)})^2} 
$$

```matlab
function J = computeCost(X, y, theta)

m = length(y); %数据量

J = 0;

predictions = X*theta; %h_θ*x^{(i)}

sqrErrors = (predictions-y).^2;

J = 1/(2*m)*sum(sqrErrors);

%   另一种表示方法
%   J = 1/(2*m)*((X*theta)-y)'*((X*theta)-y);

end;
```

```matlab
data = load('ex1data1.txt');%data里有两列数据
y = data(:, 2);%取第二列数据
m = length(y);%数据量 97
X = [ones(m, 1), data(:,1)]; %取第一列数据,左边扩列1列全1
theta = zeros(2, 1); %一列两行的0

J = computeCost(X, y, theta);
```
[cost-function](https://stackoverflow.com/questions/22625396/cost-function-linear-regression-trying-to-avoid-hard-coding-theta-octave)

# gradient descent (梯度下降)

$$
θ_j:=θ_j-α\frac {1} {m} \sum_{i=1}^{m} {(h(θ)x^{(i)}-y^{(i)})\times{x_j^{(i)}}}
\\
for \ j:=0...n
$$

```matlab
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)

m = length(y); 

J_history = zeros(num_iters, 1);

    for iter = 1:num_iters

%    theta(1) = theta(1) - alpha / m * sum(X * theta_s - y);      
%    theta(2) = theta(2) - alpha / m * sum((X * theta_s - y) .* X(:,2));    
% 必须同时更新theta(1)和theta(2)，所以不能用X * theta,而要用theta_s存储上次结果。
%    theta_s=theta;

        predictions = X*theta;
        sqrErrors = predictions-y;
        theta = theta - alpha*(1/m)* (sqrErrors' * X)';

        J_history(iter) = computeCost(X, y, theta);

    end

end
```

```matlab
data = load('ex1data1.txt');%data里有两列数据
y = data(:, 2);%取第二列数据
m = length(y);%数据量 97
X = [ones(m, 1), data(:,1)]; %取第一列数据,左边扩列1列全1
theta = zeros(2, 1); %一列两行的0

iterations = 1500;%迭代数
alpha = 0.01;%学习速率

theta = gradientDescent(X, y, theta, alpha, iterations);
```

[gradient-descent-implementation-in-octave](https://stackoverflow.com/questions/10591343/gradient-descent-implementation-in-octave)


# feature normalization(特征标准化)

标准差
$$
 std (X) = sqrt ( 1/(N-1) SUM_i (X(i) - mean(X))^2 )
$$

$$
x_i := \frac{x_i − μ_i}{s_i} 
$$

```matlab
function [X_norm, mu, sigma] = featureNormalize(X)

X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));

  mu = mean(X);       %  mean value
  sigma = std(X);     %  standard deviation
  X_norm = (X-mu) ./ sigma;

%   X_norm  = (X - repmat(mu,size(X,1),1)) ./  repmat(sigma,size(X,1),1);

%   X_norm = (X .- mean(X,1)) ./ std(X,0,1);

end

```

# compute cost multi(多变量的成本函数)

![](../week2/多变量cost%20function.png)

```matlab
function J = computeCostMulti(X, y, theta)

m = length(y); % number of training examples

J = 0;

m = size(X,1);
predictions = X*theta;
errors = predictions-y;
J = 1/(2*m)*(errors)'*errors;

%   J = sum((X * theta - y).^2) / (2*m);   

end
```


# gradient descent multi(多变量的梯度下降)

```matlab
function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)

m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters

        predictions = X*theta;
        errors = (predictions-y);
        sums = X'*errors;
        delta = 1/m*sums;
        theta = theta - alpha*delta;
%   theta = theta - alpha / m * X' * (X * theta - y);

    J_history(iter) = computeCostMulti(X, y, theta);

end

end
```

# normal equation(正规方程)

$$
θ=(X^T X)^{−1} \times X^Ty
$$

```matlab
function [theta] = normalEqn(X, y)

theta = zeros(size(X, 2), 1);

theta = pinv( X' * X ) * X' * y;

end
```

